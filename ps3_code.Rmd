---
title: "Problem Set #3"
author: "Anaya Hall & Christian Miller"
date: "Due approximately April 6th"
output: pdf_document
fontsize: 11pt
geometry: margin=.75in 
---

```{r setup, include=FALSE}

rm(list = ls())
# Setup
knitr::opts_chunk$set(echo = TRUE, cache = T)
# Options
options(stringsAsFactors = F)
# Packages
library(pacman)
p_load(knitr, kableExtra, tidyverse, dplyr, readr, magrittr, ggplot2, readxl, ascii, sandwich, tinytec)

```

# Wage Regressions - Blackburn and Neumark (QJE 1992)
The goal of this problem set is to explore some **tests for heteroskedasticity** and explore **the fixes** discussed in class.


## Question 1:
**Read the data into R. Plot the series and make sure your data are read in correctly.**

```{r read_data, message=FALSE}

# Read in CSV as data.frame
wage_df <- readr::read_csv("nls80.csv")

# Select only the variables in our model
wage_df %<>% select(wage, exper, tenure, married, south, urban, black, educ)
```


``` {r plot_series, message = FALSE}
# Plot the variables in our model
ggplot(data = gather(wage_df), aes(x = value)) +
geom_histogram() +
facet_wrap(~ key, scales = "free") +
ggtitle("Histograms of Wage Data variables") +
ylab("Count") +
xlab("Value") + theme_minimal()
```

So far, everthing looks good.

## Question 2: Exploring Heteroskedasticity

Model (1) :

$log(wage) = \beta_0 + exper \cdot \beta_1 + tenure \cdot \beta_2 + married \cdot \beta_3 + south \cdot \beta_4 + urban \cdot \beta_5 + black \cdot \beta_6 + educ \cdot \beta_7 + \epsilon$

### (a) Estimate model (1) via OLS

First, load our OLS function created in Problem Sets 1 & 2.

```{r OLS function}

# Function to convert tibble, data.frame, or tbl_df to matrix
to_matrix <- function(the_df, vars) {
  # Create a matrix from variables in var
  new_mat <- the_df %>%
    #Select the columns given in 'vars'
    select_(.dots = vars) %>%
    # Convert to matrix
    as.matrix()
  # Return 'new_mat'
  return(new_mat)
}

ols <- function(data, y_data, X_data, intercept = T, H0 = 0, two_tail = T, alpha = 0.05) {
  # Function setup ----
    # Require the 'dplyr' package
    require(dplyr)
  
  # Create dependent and independent variable matrices ----
    # y matrix
    y <- to_matrix (the_df = data, vars = y_data)
    # X matrix
    X <- to_matrix (the_df = data, vars = X_data)
      # If 'intercept' is TRUE, then add a column of ones
      if (intercept == T) {
      X <- cbind(1,X)
      colnames(X) <- c("intercept", X_data)
      }
 
  # Calculate b, y_hat, and residuals ----
    b <- solve(t(X) %*% X) %*% t(X) %*% y
    y_hat <- X %*% b
    e <- y - y_hat
    
  # Useful -----
    n <- nrow(X) # number of observations
    k <- ncol(X) # number of independent variables
    dof <- n - k # degrees of freedom
    i <- rep(1,n) # column of ones for demeaning matrix
    A <- diag(i) - (1 / n) * i %*% t(i) # demeaning matrix
    y_star <- A %*% y # for SST
    X_star <- A %*% X # for SSM
    SST <- drop(t(y_star) %*% y_star)
    SSM <- drop(t(b) %*% t(X_star) %*% X_star %*% b)
    SSR <- drop(t(e) %*% e)
  
  # Measures of fit and estimated variance ----
    R2uc <- drop((t(y_hat) %*% y_hat)/(t(y) %*% y)) # Uncentered R^2
    R2 <- 1 - SSR/SST # Uncentered R^2
    R2adj <- 1 - (n-1)/dof * (1 - R2) # Adjusted R^2
    AIC <- log(SSR/n) + 2*k/n # AIC
    SIC <- log(SSR/n) + k/n*log(n) # SIC
    s2 <- SSR/dof # s^2
  
  # Measures of fit table ----
    mof_table_df <- data.frame(R2uc, R2, R2adj, SIC, AIC, SSR, s2)
    mof_table_col_names <- c("$R^2_\\text{uc}$", "$R^2$",
                             "$R^2_\\text{adj}$",
                             "SIC", "AIC", "SSR", "$s^2$")
    mof_table <-  mof_table_df %>% knitr::kable(
      row.names = F,
      col.names = mof_table_col_names,
      format.args = list(scientific = F, digits = 4),
      booktabs = T,
      escape = F
    )
  
  # t-test----
    # Standard error
    se <- as.vector(sqrt(s2 * diag(solve(t(X) %*% X))))
    # Vector of _t_ statistics
    t_stats <- (b - H0) / se
    # Calculate the p-values
    if (two_tail == T) {
    p_values <- pt(q = abs(t_stats), df = dof, lower.tail = F) * 2
    } else {
      p_values <- pt(q = abs(t_stats), df = dof, lower.tail = F)
    }
    # Do we (fail to) reject?
    reject <- ifelse(p_values < alpha, reject <- "Reject", reject <- "Fail to Reject")
    
    # Nice table (data.frame) of results
    ttest_df <- data.frame(
      # The rows have the coef. names
      effect = rownames(b),
      # Estimated coefficients
      coef = as.vector(b) %>% round(3),
      # Standard errors
      std_error = as.vector(se) %>% round(3),
      # t statistics
      t_stat = as.vector(t_stats) %>% round(3),
      # p-values
      p_value = as.vector(p_values) %>% round(4),
      # reject null?
      significance = as.character(reject)
      )
  
    ttest_table <-  ttest_df %>% knitr::kable(
      booktabs = T,
      format.args = list(scientific = F),
      escape = F,
      caption = "OLS Results"
    )

  # Data frame for exporting for y, y_hat, X, and e vectors ----
    export_df <- data.frame(y, y_hat, e, X) %>% tbl_df()
    colnames(export_df) <- c("y","y_hat","e",colnames(X))
  
  # Return ----
    return(list(n=n, dof=dof, b=b, vars=export_df, resid=e, R2uc=R2uc,R2=R2,
                R2adj=R2adj, AIC=AIC, SIC=SIC, s2=s2, SST=SST, SSR=SSR,
                mof_table=mof_table, ttest=ttest_table))
}
```
\newpage

```{r model1}
wage_df %<>% mutate(
  log_wage = log(wage))

model_1 <- ols(wage_df, y_data = "log_wage", 
               X_data = c("exper", "tenure", "married", "south", "urban", "black", "educ"))

model_1$ttest

```

### (b) Conduct a White test for heteroskedastic errors. 
**Use levels, interactions and second order terms only. Do we have a problem?**

_White's Test:_
Regress the squared residuals ($e^2_i$) on a constant, all variables in *$X$*, squares of all variables in *$X$* and all cross products. $n \dot R^2$ from this regression is distributed as a $\chi^2_{(p-1)}$, where p is the number of regressors in this equation including the constant. The null in this test is homoskedastic disturbances.


``` {r white_test_fxn, include = F}

white_test <- function(resid, cov_mat){
  
  cov_mat %<>% as.matrix()
  
  # Interaction matrix
  cov_n <- nrow(cov_mat)
  cov_k <- sum(seq(ncol(cov_mat)))
  int_mat <- matrix(NA, nrow = cov_n, ncol = cov_k)
  
  # Loop through all columns to create interaction matrix
  for (i in 1:ncol(cov_mat)) {
    for (j in i:ncol(cov_mat)) {
      if (i == 1) m <- j
      if (i > 1) m <- sum(seq(ncol(cov_mat), 1, -1)[1:(i-1)]) + (j - i + 1)
      int_mat[, m] <- cov_mat[, i] * cov_mat[, j]
    }
  }
  
  # Bind together with covariate matrix
  cov_mat %<>% cbind(.,int_mat)
  # Make sure unique (see documentation for MARGIN = 2)
  cov_mat %<>% unique(MARGIN = 2)
  # Add intercept (column of ones)
  cov_mat %<>% cbind(1,.)

  # Outcome var ('y') is squared residual
  y_data <- resid^2
  
  # y-hat for residual regression = X*beta  
  y_hat <- cov_mat %*% solve(t(cov_mat) %*% cov_mat) %*% t(cov_mat) %*% y_data 
  
  # Calculate SSM and SST for R^2
  SSM <- sum((y_hat - mean(y_data))^2)
  SST <- sum((y_data - mean(y_data))^2)
  
  # Calculate White test statistic = R^2 * n
  test_stat <- SSM / SST * cov_n
  # Calculate pvalue
  pvalue <- 1 - pchisq(test_stat, df = ncol(cov_mat))
  
  
  return(list(PValue = pvalue, TestStat = test_stat, dof=ncol(cov_mat)))
  
  # white test results
  # whitetest_df <- data.frame()
  # 
  # 
  # white_table <-  whitetest_df %>% knitr::kable(
  #     booktabs = T,
  #     format.args = list(scientific = F),
  #     escape = F,
  #     caption = "White Test")
  
}


```



```{r 2b}
# Prep for white function
resid <- model_1$resid
cov_mat <- wage_df %>% select(exper, tenure, married, south, urban, black, educ)

# Run White Test
white_test(resid, cov_mat)
```
There is some evidence for heteroskedasticity (probabilty is 0.0678), though not significant at 5% significance level.


### (c) Goldfeld - Quandt Test for heteroskedastic errors
*Use the tenure variable, leaving out the 235 observations in the middle. Do we have a problem?*

_Goldfeld-Quant Test:_
Intuition: The disturbances for two distinct groups of observations vary.

Process: Rank observations by x, separate into groups of high and low variances, then calculate test statistic:

$ F_[n_1-k, n_2-k] = \frac {e'_1e_1/n_1 -k}{e'_2e_2/n_2 -k} $


``` {r goldfeldquant_fxn}

GQ_test <- function(x1, x2) {
  
  
  
}

```

```{r 2c}


```

### (d) Breusch-Pagan Test for heteroskedastic errors
*Use all of the covariates as a simple linear combination. Do we have a problem?*

```{r breushpagan_fxn}

```

``` {r 2d}

```

## Question 3: The Delta Method

See Ed's notes here: [http://edrub.in/ARE212/section09.html#route_2:_delta_method]

Model (2) :

$log(wage) = \beta_0 + exper \cdot \beta_1 + tenure \cdot \beta_2 + married \cdot \beta_3 + south \cdot \beta_4 + urban \cdot \beta_5 + black \cdot \beta_6 + educ \cdot \beta_7 + \epsilon $

