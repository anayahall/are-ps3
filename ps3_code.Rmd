---
title: "Problem Set #3"
author: "Anaya Hall & Christian Miller"
date: "Due approximately April 6th"
output: pdf_document
fontsize: 11pt
geometry: margin=.75in 
---

```{r setup, include=FALSE}

rm(list = ls())
# Setup
knitr::opts_chunk$set(echo = TRUE, cache = T)
# Options
options(stringsAsFactors = F)
# Packages
library(pacman)
p_load(knitr, kableExtra, tidyverse, dplyr, readr, magrittr, ggplot2, readxl, ascii, sandwich, tinytex)

```

# Wage Regressions - Blackburn and Neumark (QJE 1992)
The goal of this problem set is to explore some **tests for heteroskedasticity** and explore **the fixes** discussed in class.


## Question 1:
**Read the data into R. Plot the series and make sure your data are read in correctly.**

```{r read_data, message=FALSE}

# Read in CSV as data.frame
wage_df <- readr::read_csv("nls80.csv")

# Select only the variables in our model
wage_df %<>% select(lwage, wage, exper, tenure, married, south, urban, black, educ)
```


``` {r plot_series, message = FALSE}
# Plot the variables in our model
ggplot(data = gather(wage_df), aes(x = value)) +
  geom_histogram() +
  facet_wrap(~ key, scales = "free") +
  ggtitle("Histograms of Wage Data variables") +
  ylab("Count") +
  xlab("Value") + theme_minimal()
```

So far, everthing looks good.

## Question 2: Exploring Heteroskedasticity

Model (1) :

$log(wage) = \beta_0 + exper \cdot \beta_1 + tenure \cdot \beta_2 + married \cdot \beta_3 + south \cdot \beta_4 + urban \cdot \beta_5 + black \cdot \beta_6 + educ \cdot \beta_7 + \epsilon$

### (a) Estimate model (1) via OLS

First, load our OLS function created in Problem Sets 1 & 2.

```{r OLS function}

# Function to convert tibble, data.frame, or tbl_df to matrix
to_matrix <- function(the_df, vars) {
  # Create a matrix from variables in var
  new_mat <- the_df %>%
    #Select the columns given in 'vars'
    select_(.dots = vars) %>%
    # Convert to matrix
    as.matrix()
  # Return 'new_mat'
  return(new_mat)
}

ols <- function(data, y_data, X_data, intercept = T, H0 = 0, two_tail = T, alpha = 0.05) {
  # Function setup ----
    # Require the 'dplyr' package
    require(dplyr)
  
  # Create dependent and independent variable matrices ----
    # y matrix
    y <- to_matrix (the_df = data, vars = y_data)
    # X matrix
    X <- to_matrix (the_df = data, vars = X_data)
      # If 'intercept' is TRUE, then add a column of ones
      if (intercept == T) {
      X <- cbind(1,X)
      colnames(X) <- c("intercept", X_data)
      }
 
  # Calculate b, y_hat, and residuals ----
    b <- solve(t(X) %*% X) %*% t(X) %*% y
    y_hat <- X %*% b
    e <- y - y_hat
    
  # Useful -----
    n <- nrow(X) # number of observations
    k <- ncol(X) # number of independent variables
    dof <- n - k # degrees of freedom
    i <- rep(1,n) # column of ones for demeaning matrix
    A <- diag(i) - (1 / n) * i %*% t(i) # demeaning matrix
    y_star <- A %*% y # for SST
    X_star <- A %*% X # for SSM
    SST <- drop(t(y_star) %*% y_star)
    SSM <- drop(t(b) %*% t(X_star) %*% X_star %*% b)
    SSR <- drop(t(e) %*% e)
  
  # Measures of fit and estimated variance ----
    R2uc <- drop((t(y_hat) %*% y_hat)/(t(y) %*% y)) # Uncentered R^2
    R2 <- 1 - SSR/SST # Uncentered R^2
    R2adj <- 1 - (n-1)/dof * (1 - R2) # Adjusted R^2
    AIC <- log(SSR/n) + 2*k/n # AIC
    SIC <- log(SSR/n) + k/n*log(n) # SIC
    s2 <- SSR/dof # s^2
  
  # Measures of fit table ----
    mof_table_df <- data.frame(R2uc, R2, R2adj, SIC, AIC, SSR, s2)
    mof_table_col_names <- c("$R^2_\\text{uc}$", "$R^2$",
                             "$R^2_\\text{adj}$",
                             "SIC", "AIC", "SSR", "$s^2$")
    mof_table <-  mof_table_df %>% knitr::kable(
      row.names = F,
      col.names = mof_table_col_names,
      format.args = list(scientific = F, digits = 4),
      booktabs = T,
      escape = F
    )
  
  # t-test----
    # Standard error
    se <- as.vector(sqrt(s2 * diag(solve(t(X) %*% X))))
    # Vector of _t_ statistics
    t_stats <- (b - H0) / se
    # Calculate the p-values
    if (two_tail == T) {
    p_values <- pt(q = abs(t_stats), df = dof, lower.tail = F) * 2
    } else {
      p_values <- pt(q = abs(t_stats), df = dof, lower.tail = F)
    }
    # Do we (fail to) reject?
    reject <- ifelse(p_values < alpha, reject <- "Reject", reject <- "Fail to Reject")
    
    # Nice table (data.frame) of results
    ttest_df <- data.frame(
      # The rows have the coef. names
      effect = rownames(b),
      # Estimated coefficients
      coef = as.vector(b) %>% round(3),
      # Standard errors
      std_error = as.vector(se) %>% round(3),
      # t statistics
      t_stat = as.vector(t_stats) %>% round(3),
      # p-values
      p_value = as.vector(p_values) %>% round(4),
      # reject null?
      significance = as.character(reject)
      )
  
    ttest_table <-  ttest_df %>% knitr::kable(
      booktabs = T,
      format.args = list(scientific = F),
      escape = F,
      caption = "OLS Results"
    )

  # Data frame for exporting for y, y_hat, X, and e vectors ----
    export_df <- data.frame(y, y_hat, e, X) %>% tbl_df()
    colnames(export_df) <- c("y","y_hat","e",colnames(X))
  
  # Return ----
    return(list(n=n, dof=dof, b=b, vars=export_df, R2uc=R2uc,R2=R2,
                R2adj=R2adj, AIC=AIC, SIC=SIC, s2=s2, SST=SST, SSR=SSR,
                mof_table=mof_table, ttest=ttest_table))
}
```
\newpage

```{r model1}
model_1 <- ols(wage_df, y_data = "lwage", 
               X_data = c("exper", "tenure", "married", "south", "urban", "black", "educ"))

model_1$ttest

```

### (b) Conduct a White test for heteroskedastic errors. 
**Use levels, interactions and second order terms only. Do we have a problem?**

_White's Test:_
Regress the squared residuals ($e^2_i$) on a constant, all variables in *$X$*, squares of all variables in *$X$* and all cross products. $n \dot R^2$ from this regression is distributed as a $\chi^2_{(p-1)}$, where p is the number of regressors in this equation including the constant. The null in this test is homoskedastic disturbances.


``` {r white_test_fxn, include = F}

white_test <- function(resid, cov_mat){
  
  cov_mat %<>% as.matrix()
  
  # Interaction matrix
  cov_n <- nrow(cov_mat)
  cov_k <- sum(seq(ncol(cov_mat)))
  int_mat <- matrix(NA, nrow = cov_n, ncol = cov_k)
  
  # Loop through all columns to create interaction matrix
  for (i in 1:ncol(cov_mat)) {
    for (j in i:ncol(cov_mat)) {
      if (i == 1) m <- j
      if (i > 1) m <- sum(seq(ncol(cov_mat), 1, -1)[1:(i-1)]) + (j - i + 1)
      int_mat[, m] <- cov_mat[, i] * cov_mat[, j]
    }
  }
  
  # Bind together with covariate matrix
  cov_mat %<>% cbind(.,int_mat)
  # Make sure unique (see documentation for MARGIN = 2)
  cov_mat %<>% unique(MARGIN = 2)
  # Add intercept (column of ones)
  cov_mat %<>% cbind(1,.)

  # Outcome var ('y') is squared residual
  y_data <- resid^2
  
  # y-hat for residual regression = X*beta  
  y_hat <- cov_mat %*% solve(t(cov_mat) %*% cov_mat) %*% t(cov_mat) %*% y_data 
  
  # Calculate SSM and SST for R^2
  SSM <- sum((y_hat - mean(y_data))^2)
  SST <- sum((y_data - mean(y_data))^2)
  
  # Calculate White test statistic = R^2 * n
  test_stat <- SSM / SST * cov_n
  # Calculate pvalue
  pvalue <- 1 - pchisq(test_stat, df = (ncol(cov_mat)-1)) #dof is p-1 yes?
  
  
  return(list(PValue = pvalue, TestStat = test_stat, dof=ncol(cov_mat)))
  
  # white test results
  # whitetest_df <- data.frame()
  # 
  # 
  # white_table <-  whitetest_df %>% knitr::kable(
  #     booktabs = T,
  #     format.args = list(scientific = F),
  #     escape = F,
  #     caption = "White Test")
  
}


```



```{r 2b}
# Prep for white function
resid <- model_1$vars$e
cov_mat <- wage_df %>% select(exper, tenure, married, south, urban, black, educ)

# Run White Test
white_test(resid, cov_mat)
```
There is some evidence for heteroskedasticity (probabilty is 0.0535), though not significant at 5% significance level.


### (c) Goldfeld - Quandt Test for heteroskedastic errors
*Use the tenure variable, leaving out the 235 observations in the middle. Do we have a problem?*


_Goldfeld-Quant Test:_
Intuition: The disturbances for two distinct groups of observations vary.

Process: Rank observations by x (variable of interest) and separate into groups of high and low variances. In this case we will remove the middle 235 observations (increasing the power of the test). For the two samples, run the least squares moel and record the residuals from each regression. Then calculate test statistic: $F_{[n_1-k, \space n_2-k]}= \frac{e_1^`e_1/(n_1-k)}{e_2^`e_2/(n_2-k)}$


``` {r goldfeldquant_fxn}

GQ_test <- function(e1, e2, k) {
  n1 <- length(e1)
  n2 <- length(e2)
  
  SSE1 <- (t(e1) %*% e1)/(n1-k)
  SSE2 <- (t(e2) %*% e2)/(n2-k)
  test_stat <- SSE1 / SSE2
  
  pvalue <- 1 - pf(test_stat, n1-k, n2-k)
  
  return(list(
    "Test Statistic" = test_stat,
    "P-Value" = pvalue
  ))
}

```

```{r 2c}
# Prep for test
# Rank by tenure
wage_df %<>% arrange(tenure)
# Splitting the data, removing the middle 235 observations
wage_df1 <- wage_df[1:350,]
wage_df2 <- wage_df[586:935,]
# Run two regressions saving their residuals
gq_resid_1 <- ols(wage_df1, y_data = "lwage", 
               X_data = c("exper", "tenure", "married", "south", "urban", "black", "educ"))$vars$e
gq_resid_2 <- ols(wage_df2, y_data = "lwage", 
               X_data = c("exper", "tenure", "married", "south", "urban", "black", "educ"))$vars$e

# Run GQ Test
GQ_test(gq_resid_1, gq_resid_2, k=7)

```
According to the Goldfeld-Quandt Test (by tenure), yes, we have a significant heteroskedasticity problem.

### (d) Breusch-Pagan Test for heteroskedastic errors
*Use all of the covariates as a simple linear combination. Do we have a problem?*

```{r breushpagan_fxn}
BP_test <- function(e, k) {
  n <- length(e)
  
  SSE <- sum((e-mean(e))^2)
  test_stat <- (1/2)*SSE
  
  pvalue <- 1 - pchisq(test_stat, df = k)
  
  return(list(
    "Test Statistic" = test_stat,
    "P-Value" = pvalue
  ))
}

```

``` {r 2d}
# Prep for test
# Create new df w/ new variables
num <- model_1$vars$e^2
denom <- (t(model_1$vars$e) %*% model_1$vars$e)/nrow(wage_df) %>% as.numeric()
wage_df %<>% mutate(bp_y = num/denom)

# Run the regression with the new y variable
bp_resid <- ols(wage_df, y_data = "bp_y", 
               X_data = c("exper", "tenure", "married", "south", "urban", "black", "educ"))$vars$e

# Run GQ Test
BP_test(bp_resid, k=7)

```
Very low p-value. So, this test further validates. But, something may be broken.e


### (e) White Robust Standard Errors

```{r Spherical varcov fxn}
# Function for OLS coefficient estimates
b_ols <- function(y, X) {
  # Calculate beta hat
  beta_hat <- solve(t(X) %*% X) %*% t(X) %*% y
  # Return beta_hat
  return(beta_hat)
}

# Function for OLS coef., SE, t-stat, and p-value
vcov_ols <- function(data, y_var, X_vars, intercept = T) {
  # Turn data into matrices
  y <- to_matrix(data, y_var)
  X <- to_matrix(data, X_vars)
  # Add intercept
  if (intercept == T) X <- cbind(1, X)
  # Calculate n and k for degrees of freedom
  n <- nrow(X)
  k <- ncol(X)
  # Estimate coefficients
  b <- b_ols(y, X)
  # Update names
  if (intercept == T) rownames(b)[1] <- "Intercept"
  # Calculate OLS residuals
  e <- y - X %*% b
  # Calculate s^2
  s2 <- (t(e) %*% e) / (n-k)
  s2 %<>% as.numeric()
  # Inverse of X'X
  XX_inv <- solve(t(X) %*% X)
  # Return the results
  return(as.numeric(s2) * XX_inv)
}  
```


```{r Robust varcov fxn}
vcov_white <- function(data, y_var, X_vars, intercept = T) {
  # Turn data into matrices
  y <- to_matrix(data, y_var)
  X <- to_matrix(data, X_vars)
  # Add intercept
  if (intercept == T) X <- cbind(1, X)
  # Calculate n and k for degrees of freedom
  n <- nrow(X)
  k <- ncol(X)
  # Estimate coefficients
  b <- b_ols(y, X)
  # Update names
  if (intercept == T) rownames(b)[1] <- "Intercept"
  # Calculate OLS residuals
  e <- y - X %*% b
  # Inverse of X'X
  XX_inv <- solve(t(X) %*% X)
  # For each row, calculate x_i' x_i e_i^2; then sum
  sigma_hat <- lapply(X = 1:n, FUN = function(i) {
    # Define x_i
    x_i <- matrix(as.vector(X[i,]), nrow = 1)
    # Return x_i' x_i e_i^2
    return(t(x_i) %*% x_i * e[i]^2)
  }) %>% Reduce(f = "+", x = .)
  # Return the results
  return(XX_inv %*% sigma_hat %*% XX_inv)
}
```

```{r Spherical Variance}
# Comparing spherical variance
vcov_ols(data = wage_df,
  y_var = "lwage",
  X_vars = c("exper", "tenure", "married", "south", "urban", "black", "educ")) %>%
  diag() %>% sqrt()

```

```{r White's robust}
# With White's robust
vcov_white(data = wage_df,
  y_var = "lwage",
  X_vars = c("exper", "tenure", "married", "south", "urban", "black", "educ")) %>%
  diag() %>% sqrt()

```
The "heteroskedasitc" robust standard errors are about the same as the traditional OLS standard erros. This leads me to believe that this doesn't solve the problem.

### (f) Two-step FGLS estimation procedure
```{r 2SFGLS}
# Add e^2 from earlier to our dataframe
wage_df3 <-select(wage_df, - bp_y)

#should I put some loop?
model_3 <- ols(wage_df3, y_data = "lwage", 
               X_data = c("exper", "tenure", "married", "south", "urban", "black", "educ"))


wage_df3 %<>% mutate(e2 = model_3$vars$e^2)
fgls_predict <- bp_resid <- ols(wage_df3, y_data = "e2", 
               X_data = c("exper", "tenure", "married", "south", "urban", "black", "educ"))$vars$y_hat
weights <- 1/sqrt(fgls_predict)

wage_df3 <- wage_df3 * weights

# Grab the new variances
vcov_ols(data = wage_df3,
  y_var = "lwage",
  X_vars = c("exper", "tenure", "married", "south", "urban", "black", "educ")) %>%
  diag() %>% sqrt()

# Could do a loop, but I don't know what threshold would be reasonable
```


## Question 3: The Delta Method

See Ed's notes here: [http://edrub.in/ARE212/section09.html#route_2:_delta_method]


